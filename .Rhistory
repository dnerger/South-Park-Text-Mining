southparkPrevFit$theta
apply(southparkPrevFit$theta, 1, max)
apply(southparkPrevFit$theta, 1, whiich.max)
apply(southparkPrevFit$theta, 1, which.max)
plot(xyyy)
xyyy<- apply(southparkPrevFit$theta, 1, which.max)
plot(xyyy)
lda_show <- toLDAvis(southparkPrevFit, docs = out$documents)
lda_show <- toLDAvis(southparkPrevFit, docs = out$documents)
plot(xyyy, xlab("Episode"), ylab("Topic"))
plot(xyyy, xlabel="Episode", ylabel="Index")
xyyy<- apply(southparkPrevFit$theta, 1, which.max)
plot(xyyy, xlabel="Episode", ylabel="Index")
p <- ggplot(southparkPrevFit$theta, aes(x, y)) + geom_point()
topic_episodes <- data.matrix(southparkPrevFit$theta)
View(topic_episodes)
p <- ggplot(southparkPrevFit$theta) + geom_point()
p + labs(colour = "Cylinders")
p <- ggplot(topic_episodes) + geom_point()
p <- ggplot(topic_episodes) + geom_point()
topic_episodes <- data.frame(southparkPrevFit$theta)
p <- ggplot(topic_episodes) + geom_point()
p + labs(colour = "Cylinders")
xyyy
topic_episodes <- data.frame(xyyy)
p <- ggplot(xyyy) + geom_point()
topic_episodes <- data.frame(xyyy)
p <- ggplot(topic_episodes) + geom_point()
p + labs(colour = "Cylinders")
p <- ggplot(topic_episodes,aes(names(xyyy), xyyy)) + geom_point()
p + labs(colour = "Cylinders")
p <- ggplot(topic_episodes,aes(dim(xyyy), xyyy)) + geom_point()
p + labs(colour = "Cylinders")
p <- ggplot(topic_episodes,aes(seq_along(xyyy), xyyy)) + geom_point()
p + labs(colour = "Cylinders")
p <- ggplot(topic_episodes,aes(seq_along(xyyy), xyyy, xyyy)) + geom_point()
p + labs(colour = "Cylinders")
p + xlab("Episode") + ylab("Topic")
p + xlab("Episode") + ylab("Topic")# scale_colour_gradientn(colours=rainbow(4))
p + xlab("Episode") + ylab("Topic")+ scale_colour_gradientn(colours=rainbow(4))
p + xlab("Episode") + ylab("Topic")+ scale_colour_gradientn(colours=rainbow(4))
p <- ggplot(topic_episodes,aes(seq_along(xyyy), xyyy, colour=xyyy)) + geom_point()
p + xlab("Episode") + ylab("Topic")+ scale_colour_gradientn(colours=rainbow(4))
p + xlab("Episode") + ylab("Topic")
p + xlab("Episode") + ylab("Topic")+theme(legend.position="none")
p + xlab("Episode") + ylab("Topic")+theme(legend.position="none")+scale_colour_brewer(palette = "Greens")
p + xlab("Episode") + ylab("Topic")+theme(legend.position="none")
by.episode <- NULL
for(g in seq_along(episodes)){
subset <- dialogue[dialogue$ES==episodes[g], ]
subset <- subset[complete.cases(subset), ]
text <- str_c(subset$Line, collapse=" ")
if (subset$Season[1]==18){
row <- data.frame(episode.number=g, episode.code=episodes[g], season=subset$Season[1], text=text)
by.episode <- rbind(by.episode, row)
}
}
processed <- textProcessor(by.episode$text, metadata = by.episode)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 0, upper.thresh = 3)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 0, upper.thresh = 5)
southparkPrevFit <- stm(documents = out$documents, vocab = out$vocab,K=5, data = out$meta)
plot(southparkPrevFit, type = "summary", main="Top Topics of Season 18")
lda_show <- toLDAvis(southparkPrevFit, docs = out$documents)
xyyy<- apply(southparkPrevFit$theta, 1, which.max)
topic_episodes <- data.frame(xyyy)
p <- ggplot(topic_episodes,aes(seq_along(xyyy), xyyy, colour=xyyy)) + geom_point()
p + xlab("Episode") + ylab("Topic")+theme(legend.position="none")
southparkPrevFit <- stm(documents = out$documents, vocab = out$vocab,K=3, data = out$meta)
xyyy<- apply(southparkPrevFit$theta, 1, which.max)
topic_episodes <- data.frame(xyyy)
p <- ggplot(topic_episodes,aes(seq_along(xyyy), xyyy, colour=xyyy)) + geom_point()
p + xlab("Episode") + ylab("Topic")+theme(legend.position="none")
southparkPrevFit <- stm(documents = out$documents, vocab = out$vocab,K=4, data = out$meta)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 0, upper.thresh = 25)
processed <- textProcessor(by.episode$text, metadata = by.episode)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
southparkPrevFit <- stm(documents = out$documents, vocab = out$vocab,K=4, data = out$meta)
xyyy<- apply(southparkPrevFit$theta, 1, which.max)
topic_episodes <- data.frame(xyyy)
p <- ggplot(topic_episodes,aes(seq_along(xyyy), xyyy, colour=xyyy)) + geom_point()
p + xlab("Episode") + ylab("Topic")+theme(legend.position="none")
lda_show <- toLDAvis(southparkPrevFit, docs = out$documents)
plot(southparkPrevFit, type = "summary", main="Top Topics of Season 18")
save.image("~/Data Analysis/files/southpark/rdata.RData")
lda_show <- toLDAvis(southparkPrevFit, docs = out$documents)
xyyy<- apply(southparkPrevFit$theta, 1, which.max)
topic_episodes <- data.frame(xyyy)
p <- ggplot(topic_episodes,aes(seq_along(xyyy), xyyy, colour=xyyy)) + geom_point()
p + xlab("Episode") + ylab("Topic")+theme(legend.position="none")
lda_show <- toLDAvis(southparkPrevFit, docs = out$documents)
ent1
pers1
org1
full_dialogue = paste(by.episode$text[1], collapse = " ")
dialogue_annotations <- annotate(as.String(full_dialogue), pipeline)
dialogue_doc <- AnnotatedPlainTextDocument(full_dialogue, dialogue_annotations)
full_dialogue = paste(by.episode$text[1], collapse = " ")
dialogue_annotations <- annotate(as.String(full_dialogue), pipeline)
dialogue_doc <- AnnotatedPlainTextDocument(full_dialogue, dialogue_annotations)
org1 <-entities(dialogue_doc, kind = "organization")
loc1 <-entities(dialogue_doc, kind = "location")
pers1<-entities(dialogue_doc, kind = "person")
org1
loc1
pers1
ent1<-entities(dialogue_doc)
ent1
full_dialogue = paste(by.episode$text[1], collapse = " ")
dialogue_annotations <- annotate(as.String(full_dialogue), pipeline)
library(NLP)
library(openNLP)
library(magrittr)
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
person_ann <- Maxent_Entity_Annotator(kind = "person")
location_ann <- Maxent_Entity_Annotator(kind = "location")
organization_ann <- Maxent_Entity_Annotator(kind = "organization")
pipeline <- list(sent_ann,
word_ann,
person_ann,
location_ann,
organization_ann)
entities <- function(doc, kind) {
s <- doc$content
a <- annotations(doc)[[1]]
if(hasArg(kind)) {
k <- sapply(a$features, `[[`, "kind")
s[a[k == kind]]
} else {
s[a[a$type == "entity"]]
}
}
full_dialogue = paste(by.episode$text[1], collapse = " ")
dialogue_annotations <- annotate(as.String(full_dialogue), pipeline)
dialogue_doc <- AnnotatedPlainTextDocument(full_dialogue, dialogue_annotations)
org1 <-entities(dialogue_doc, kind = "organization")
loc1 <-entities(dialogue_doc, kind = "location")
pers1<-entities(dialogue_doc, kind = "person")
org1
loc1
pers1
ent1<-entities(dialogue_doc)
ent1
full_dialogue <- paste(by.episode$text[1], collapse = " ")
dialogue_annotations <- annotate(as.String(full_dialogue), pipeline)
lda_show <- toLDAvis(southparkPrevFit, docs = out$documents)
by.episode <- NULL
for(g in seq_along(episodes)){
subset <- dialogue[dialogue$ES==episodes[g], ]
subset <- subset[complete.cases(subset), ]
text <- str_c(subset$Line, collapse=" ")
# if (subset$Season[1]==18){
row <- data.frame(episode.number=g, episode.code=episodes[g], season=subset$Season[1], text=text)
by.episode <- rbind(by.episode, row)
#}
}
corpus <- Corpus(DataframeSource(by.episode), readerControl=list(reader=myReader))
corpus <- tm_map(corpus,content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removePunctuation))
corpus <- tm_map(corpus, content_transformer(removeNumbers))
corpus <- tm_map(corpus, content_transformer(stripWhitespace))
corpus <- tm_map(corpus, removeWords, stopwords("english"))
plot(southparkPrevFit, type = "summary", main="Top Topics of Season 18")
by.episode <- NULL
for(g in seq_along(episodes)){
subset <- dialogue[dialogue$ES==episodes[g], ]
subset <- subset[complete.cases(subset), ]
text <- str_c(subset$Line, collapse=" ")
if (subset$Season[1]==18){
row <- data.frame(episode.number=g, episode.code=episodes[g], season=subset$Season[1], text=text)
by.episode <- rbind(by.episode, row)
}
}
processed <- textProcessor(by.episode$text, metadata = by.episode)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 0, upper.thresh = 25)
southparkPrevFitSeason18 <- stm(documents = out$documents, vocab = out$vocab,K=4, data = out$meta)
plot(southparkPrevFitSeason18, type = "summary", main="Top Topics of Season 18")
lda_show <- toLDAvis(southparkPrevFitSeason18, docs = out$documents)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 0, upper.thresh = 4)
southparkPrevFitSeason18 <- stm(documents = out$documents, vocab = out$vocab,K=4, data = out$meta)
lda_show <- toLDAvis(southparkPrevFitSeason18, docs = out$documents)
xyyy<- apply(southparkPrevFitSeason18$theta, 1, which.max)
topic_episodes <- data.frame(xyyy)
p <- ggplot(topic_episodes,aes(seq_along(xyyy), xyyy, colour=xyyy)) + geom_point()
p + xlab("Episode") + ylab("Topic")+theme(legend.position="none")
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 0, upper.thresh = 8)
southparkPrevFitSeason18 <- stm(documents = out$documents, vocab = out$vocab,K=4, data = out$meta)
xyyy<- apply(southparkPrevFitSeason18$theta, 1, which.max)
topic_episodes <- data.frame(xyyy)
p <- ggplot(topic_episodes,aes(seq_along(xyyy), xyyy, colour=xyyy)) + geom_point()
p + xlab("Episode") + ylab("Topic")+theme(legend.position="none")
lda_show <- toLDAvis(southparkPrevFitSeason18, docs = out$documents)
transcripts_3 <- dialogue[which(dialogue$Character %in% character_appear$value[1:40]), ]
speaker_scene_matrix <- transcripts_3 %>%
acast(Character ~ Episode, fun.aggregate = length)
data_matrix <- as.matrix(t(speaker_scene_matrix))
total_occurrences <- colSums(t(speaker_scene_matrix))
co_occurrence <- t(data_matrix) %*% data_matrix
g <- graph.adjacency(co_occurrence, weighted = TRUE, mode = "undirected", diag = FALSE)
plot(g, edge.width = E(g)$weight/500000)
g <- graph.adjacency(co_occurrence,
weighted = TRUE,
diag = FALSE,
mode = "upper")
g <- simplify(g, remove.multiple = FT, remove.loops = T, edge.attr.comb = c(weight = "sum", type = "ignore"))
females <- c("Liane", "Sharon", "Sheila", "Wendy")
V(g)$gender <- ifelse(V(g)$name %in% females, "female", "male")
plot(g,
vertex.label.family = "Helvetica",
vertex.label.font = 1,
vertex.shape = "sphere",
vertex.size=total_occurrences/230,
vertex.label.cex=0.9,
vertex.color=c( "pink", "skyblue")[1+(V(g)$gender=="male")],
vertex.label.color="black",
vertex.frame.color = NA,
edge.width = E(g)$weight/300000,
edge.curved=.1,
layout=layout_in_circle)
norm <- speaker_scene_matrix / rowSums(speaker_scene_matrix)
h <- hclust(dist(norm, method = "manhattan"))
plot(h)
plot(southparkPrevFitSeason18, type = "summary", main="Top Topics of Season 18")
xyyy<- apply(southparkPrevFitSeason18$theta, 1, which.max)
topic_episodes <- data.frame(xyyy)
p <- ggplot(topic_episodes,aes(seq_along(xyyy), xyyy, colour=xyyy)) + geom_point()
p + xlab("Episode") + ylab("Topic")+theme(legend.position="none")
lda_show <- toLDAvis(southparkPrevFitSeason18, docs = out$documents)
xyyy<- apply(southparkPrevFitSeason18$theta, 1, which.max)
topic_episodes <- data.frame(xyyy)
p <- ggplot(topic_episodes,aes(seq_along(xyyy), xyyy, colour=xyyy)) + geom_point()
p + xlab("Episode") + ylab("Topic")+theme(legend.position="none")
by.episode <- NULL
for(g in seq_along(episodes)){
subset <- dialogue[dialogue$ES==episodes[g], ]
subset <- subset[complete.cases(subset), ]
text <- str_c(subset$Line, collapse=" ")
#if (subset$Season[1]==18){
row <- data.frame(episode.number=g, episode.code=episodes[g], season=subset$Season[1], text=text)
by.episode <- rbind(by.episode, row)
#}
}
processed <- textProcessor(by.episode$text, metadata = by.episode)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 0, upper.thresh = 25)
southparkPrevFit <- stm(documents = out$documents, vocab = out$vocab,K=4, data = out$meta)
southparkPrevFit <- stm(documents = out$documents, vocab = out$vocab,K=25, data = out$meta)
lda_show <- toLDAvis(southparkPrevFit, docs = out$documents)
xyyy<- apply(southparkPrevFit$theta, 1, which.max)
topic_episodes <- data.frame(xyyy)
p <- ggplot(topic_episodes,aes(seq_along(xyyy), xyyy, colour=xyyy)) + geom_point()
p + xlab("Episode") + ylab("Topic")+theme(legend.position="none")
library(stringr)
library(wordcloud)
library(RColorBrewer)
library(quanteda)
library(tm)
library(igraph)
library(ggraph)
directory <- "~/GitHub/South-Park-Text-Mining"
dir.create(directory, recursive = TRUE, showWarnings = FALSE)
setwd(directory)
dialogue <- read.csv("all-seasons.csv", stringsAsFactors=FALSE)
for(h in seq_along(dialogue[,1])) if(nchar(dialogue$Episode[h]) < 2) dialogue$Episode[h] <- paste("0", dialogue$Episode[h], sep="")
dialogue$ES <- paste(dialogue$Season, dialogue$Episode, sep=".")
for(j in c(1,5)) dialogue[,j] <- as.numeric(dialogue[,j])
dialogue <- dialogue[complete.cases(dialogue), ]
episodes <- unique(dialogue$ES)
episodes <- episodes[order(episodes)]
by.episode <- NULL
for(g in seq_along(episodes)){
subset <- dialogue[dialogue$ES==episodes[g], ]
subset <- subset[complete.cases(subset), ]
text <- str_c(subset$Line, collapse=" ")
row <- data.frame(episode.number=g, episode.code=episodes[g], season=subset$Season[1], text=text)
by.episode <- rbind(by.episode, row)
}
seasons <-unique(dialogue$Season)
seasons <-sort.int(as.numeric(as.character(seasons)))
by.season <- NULL;
for(g in seq_along(seasons)){
subset <- dialogue[dialogue$Season==seasons[g], ]
subset <- subset[complete.cases(subset), ]
text <- str_c(subset$Line, collapse=" ")
row <- data.frame( season=seasons[g], text=text)
by.season <- rbind(by.season, row)
}
#create corpus
myReader <- readTabular(mapping=list(content="text", id="season"))
#can exchange by.season with by.episode, delivers different results for tf-idf
corpus <- Corpus(DataframeSource(by.season), readerControl=list(reader=myReader))
#preprocessing
corpus <- tm_map(corpus,content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removePunctuation))
corpus <- tm_map(corpus, content_transformer(removeNumbers))
corpus <- tm_map(corpus, content_transformer(stripWhitespace))
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument, language = "english")
#dtm with tf-idf weighting
terms <-DocumentTermMatrix(corpus,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
#remove sparse terms to decrease size
terms <- removeSparseTerms(terms, 0.95)
tf_idf_mat <- as.matrix(terms)
tf_idf_v <- sort(colSums(tf_idf_mat), decreasing=TRUE)
head(tf_idf_v, 20)
#first word is  'ã,â', no idea why
tf_idf_v[1]<-0
tf_idf_wordcloud <- data.frame(word = names(tf_idf_v), value = tf_idf_v)
png(file="WordCloud-TFIDF.png",width=1000,height=700, bg="grey30")
wordcloud(tf_idf_wordcloud$word, tf_idf_wordcloud$value, col=terrain.colors(length(tf_idf_wordcloud$word), alpha=0.9), max.words=100, random.order=FALSE, rot.per=0.3 )
title(main = "Words by tf-idf score over 18 seasons of South Park", font.main = 1, col.main = "cornsilk3", cex.main = 1.5)
dev.off()
#dtm with tf-idf weighting
terms <-DocumentTermMatrix(corpus,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
#remove sparse terms to decrease size
terms <- removeSparseTerms(terms, 0.90)
tf_idf_mat <- as.matrix(terms)
tf_idf_v <- sort(colSums(tf_idf_mat), decreasing=TRUE)
head(tf_idf_v, 20)
#first word is  'ã,â', no idea why
tf_idf_v[1]<-0
tf_idf_wordcloud <- data.frame(word = names(tf_idf_v), value = tf_idf_v)
library(stringr)
library(wordcloud)
library(RColorBrewer)
library(quanteda)
library(tm)
library(igraph)
library(ggraph)
directory <- "~/GitHub/South-Park-Text-Mining"
dir.create(directory, recursive = TRUE, showWarnings = FALSE)
setwd(directory)
dialogue <- read.csv("all-seasons.csv", stringsAsFactors=FALSE)
for(h in seq_along(dialogue[,1])) if(nchar(dialogue$Episode[h]) < 2) dialogue$Episode[h] <- paste("0", dialogue$Episode[h], sep="")
dialogue$ES <- paste(dialogue$Season, dialogue$Episode, sep=".")
for(j in c(1,5)) dialogue[,j] <- as.numeric(dialogue[,j])
dialogue <- dialogue[complete.cases(dialogue), ]
episodes <- unique(dialogue$ES)
episodes <- episodes[order(episodes)]
by.episode <- NULL
for(g in seq_along(episodes)){
subset <- dialogue[dialogue$ES==episodes[g], ]
subset <- subset[complete.cases(subset), ]
text <- str_c(subset$Line, collapse=" ")
row <- data.frame(episode.number=g, episode.code=episodes[g], season=subset$Season[1], text=text)
by.episode <- rbind(by.episode, row)
}
seasons <-unique(dialogue$Season)
seasons <-sort.int(as.numeric(as.character(seasons)))
by.season <- NULL;
for(g in seq_along(seasons)){
subset <- dialogue[dialogue$Season==seasons[g], ]
subset <- subset[complete.cases(subset), ]
text <- str_c(subset$Line, collapse=" ")
row <- data.frame( season=seasons[g], text=text)
by.season <- rbind(by.season, row)
}
#create corpus
myReader <- readTabular(mapping=list(content="text", id="season"))
#can exchange by.season with by.episode, delivers different results for tf-idf
corpus <- Corpus(DataframeSource(by.season), readerControl=list(reader=myReader))
#preprocessing
corpus <- tm_map(corpus,content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removePunctuation))
corpus <- tm_map(corpus, content_transformer(removeNumbers))
corpus <- tm_map(corpus, content_transformer(stripWhitespace))
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument, language = "english")
#dtm with tf-idf weighting
terms <-DocumentTermMatrix(corpus,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
#remove sparse terms to decrease size, choose 0.95 for seasons and 0.99 for episodes
terms <- removeSparseTerms(terms, 0.95)
tf_idf_mat <- as.matrix(terms)
tf_idf_v <- sort(colSums(tf_idf_mat), decreasing=TRUE)
head(tf_idf_v, 20)
#first word is  'ã,â', no idea why
tf_idf_v[1]<-0
tf_idf_wordcloud <- data.frame(word = names(tf_idf_v), value = tf_idf_v)
png(file="WordCloud-TFIDF.png",width=1000,height=700)
wordcloud(tf_idf_wordcloud$word, tf_idf_wordcloud$value, col=terrain.colors(length(tf_idf_wordcloud$word), alpha=0.9), max.words=100, random.order=FALSE, rot.per=0.3 )
title(main = "Words by tf-idf score over 18 seasons of South Park", font.main = 1, col.main = "cornsilk3", cex.main = 1.5)
dev.off()
#build plot for n-grams
episodes_text<-paste(by.episode$text, collapse=" // ")
episodes_text<- gsub('[[:punct:] ]+',' ',episodes_text)
episodes_text <- tolower(episodes_text)
episodes_text<- removeWords(episodes_text, stopwords("english"))
ngram_dfm <- dfm(episodes_text, stem=TRUE, remove_punct=TRUE, ngrams=4:4, verbose = FALSE, concatenator=" ")
ngram_df <- data.frame(word=ngram_dfm@Dimnames$features, freq=ngram_dfm@x)
ngram_df <- arrange(ngram_df, -freq)
png(file="WordCloud-ngram.png",width=1000,height=700)
wordcloud(ngram_df$word, ngram_df$freq, col=terrain.colors(length(ngram_df$word), alpha=0.9), max.words=100, random.order=FALSE, rot.per=0.3 )
title(main = "Quadgrams over 18 seasons of South Park", font.main = 1, col.main = "cornsilk3", cex.main = 1.5)
dev.off()
#graph linking n-grams, doesn't provide insights
ngram_graph <- ngram_df %>%
filter(freq > 10) %>%
graph_from_data_frame()
set.seed(2017)
ggraph(ngram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
#additional term-network, doesn't really provide any insights
tdm <- TermDocumentMatrix(corpus,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
tdm <- removeSparseTerms(tdm, 0.4)
tdmMatrix <- as.matrix(tdm)
#remove connections below 10
tdmMatrix[tdmMatrix<10] <- 0
tdmMatrix[tdmMatrix>=10] <- 1
termMatrix <- tdmMatrix %*% t(tdmMatrix)
library(igraph)
# build a graph from the above matrix
g <- graph.adjacency(termMatrix, weighted=T, mode="undirected")
# remove loops
g <- simplify(g)
# set labels and degrees of vertices
V(g)$label <- V(g)$name
V(g)$degree <- degree(g)/30000
set.seed(3952)
layout1 <- layout.fruchterman.reingold(g)
pdf("term-network.pdf")
plot(g, layout=layout1)
dev.off()
display.brewer.all()
png(file="WordCloud-TFIDF.png",width=1000,height=700)
wordcloud(tf_idf_wordcloud$word, tf_idf_wordcloud$value, col=terrain.colors(length(tf_idf_wordcloud$word), alpha=0.9), max.words=100, random.order=FALSE, rot.per=0.3, colors = 'Spectral')
title(main = "Words by tf-idf score over 18 seasons of South Park", font.main = 1, col.main = "cornsilk3", cex.main = 1.5)
dev.off()
png(file="WordCloud-TFIDF.png",width=1000,height=700)
wordcloud(tf_idf_wordcloud$word, tf_idf_wordcloud$value, col=terrain.colors(length(tf_idf_wordcloud$word), alpha=0.3), max.words=100, random.order=FALSE, rot.per=0.3)
title(main = "Words by tf-idf score over 18 seasons of South Park", font.main = 1, col.main = "cornsilk3", cex.main = 1.5)
dev.off()
png(file="WordCloud-TFIDF.png",width=1000,height=700)
wordcloud(tf_idf_wordcloud$word, tf_idf_wordcloud$value, col=terrain.colors(length(tf_idf_wordcloud$word), alpha=1), max.words=100, random.order=FALSE, rot.per=0.3)
title(main = "Words by tf-idf score over 18 seasons of South Park", font.main = 1, col.main = "cornsilk3", cex.main = 1.5)
dev.off()
png(file="WordCloud-ngram.png",width=1000,height=700)
wordcloud(ngram_df$word, ngram_df$freq, col=terrain.colors(length(ngram_df$word), alpha=1), max.words=100, random.order=FALSE, rot.per=0.3 )
title(main = "Quadgrams over 18 seasons of South Park", font.main = 1, col.main = "cornsilk3", cex.main = 1.5)
dev.off()
png(file="WordCloud-ngram.png",width=1000,height=700)
wordcloud(ngram_df$word, ngram_df$freq, col=terrain.colors(length(ngram_df$word), alpha=1), max.words=100, random.order=FALSE, rot.per=0.3 )
title(main = "Quadgrams over 18 seasons of South Park", font.main = 1, col.main = "cornsilk3", cex.main = 1.5)
dev.off()
png(file="WordCloud-ngram.png",width=1000,height=700)
wordcloud(ngram_df$word, ngram_df$freq, col=topo.colors(length(ngram_df$word), alpha=1), max.words=100, random.order=FALSE, rot.per=0.3 )
title(main = "Quadgrams over 18 seasons of South Park", font.main = 1, col.main = "cornsilk3", cex.main = 1.5)
dev.off()
png(file="WordCloud-ngram.png",width=1000,height=700)
wordcloud(ngram_df$word, ngram_df$freq, col=heat.colors(length(ngram_df$word), alpha=1), max.words=100, random.order=FALSE, rot.per=0.3 )
title(main = "Quadgrams over 18 seasons of South Park", font.main = 1, col.main = "cornsilk3", cex.main = 1.5)
dev.off()
png(file="WordCloud-ngram.png",width=1000,height=700)
wordcloud(ngram_df$word, ngram_df$freq, col=cm.colors(length(ngram_df$word), alpha=1), max.words=100, random.order=FALSE, rot.per=0.3 )
title(main = "Quadgrams over 18 seasons of South Park", font.main = 1, col.main = "cornsilk3", cex.main = 1.5)
dev.off()
png(file="WordCloud-ngram.png",width=1000,height=700)
wordcloud(ngram_df$word, ngram_df$freq, col=topo.colors(length(ngram_df$word), alpha=1), max.words=100, random.order=FALSE, rot.per=0.3 )
title(main = "Quadgrams over 18 seasons of South Park", font.main = 1, col.main = "cornsilk3", cex.main = 1.5)
dev.off()
png(file="WordCloud-ngram.png",width=1000,height=700)
wordcloud(ngram_df$word, ngram_df$freq, col=terrain.colors(length(ngram_df$word), alpha=1), max.words=100, random.order=FALSE, rot.per=0.3 )
title(main = "Quadgrams over 18 seasons of South Park", font.main = 1, col.main = "cornsilk3", cex.main = 1.5)
dev.off()
png(file="WordCloud-ngram.png",width=1000,height=700)
wordcloud(ngram_df$word, ngram_df$freq, col=terrain.colors(length(ngram_df$word), alpha=1), max.words=100, random.order=FALSE, rot.per=0.3 )
#title(main = "Quadgrams over 18 seasons of South Park", font.main = 1, col.main = "cornsilk3", cex.main = 1.5)
dev.off()
ggraph(ngram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
# build a graph from the above matrix
g <- graph.adjacency(termMatrix, weighted=T, mode="undirected")
# remove loops
g <- simplify(g)
# set labels and degrees of vertices
V(g)$label <- V(g)$name
V(g)$degree <- degree(g)/30000
set.seed(3952)
layout1 <- layout.fruchterman.reingold(g)
pdf("term-network.pdf")
plot(g, layout=layout1)
dev.off()
#can exchange by.season with by.episode, delivers different results for tf-idf
corpus <- Corpus(DataframeSource(by.episode), readerControl=list(reader=myReader))
#preprocessing
corpus <- tm_map(corpus,content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removePunctuation))
corpus <- tm_map(corpus, content_transformer(removeNumbers))
corpus <- tm_map(corpus, content_transformer(stripWhitespace))
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument, language = "english")
#dtm with tf-idf weighting
terms <-DocumentTermMatrix(corpus,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
#remove sparse terms to decrease size, choose 0.95 for seasons and 0.99 for episodes
terms <- removeSparseTerms(terms, 0.99)
tf_idf_mat <- as.matrix(terms)
tf_idf_v <- sort(colSums(tf_idf_mat), decreasing=TRUE)
head(tf_idf_v, 20)
#first word is  'ã,â', no idea why
tf_idf_v[1]<-0
tf_idf_wordcloud <- data.frame(word = names(tf_idf_v), value = tf_idf_v)
png(file="WordCloud-TFIDF.png",width=1000,height=700)
wordcloud(tf_idf_wordcloud$word, tf_idf_wordcloud$value, col=terrain.colors(length(tf_idf_wordcloud$word), alpha=1), max.words=100, random.order=FALSE, rot.per=0.3)
title(main = "Words by tf-idf score over 18 seasons of South Park", font.main = 1, col.main = "cornsilk3", cex.main = 1.5)
dev.off()
