directory = paste("~/Data Analysis/files/", tvshow, sep="")
dir.create(directory, recursive = TRUE, showWarnings = FALSE)
setwd(directory)
dialogue <- read.csv('all-seasons.csv',stringsAsFactors = FALSE)
seasons <-unique(dialogue$Season)
seasons <-sort.int(as.numeric(as.character(seasons)))
seasons
by.season <- NULL;
for(g in seq_along(seasons)){
subset <- dialogue[dialogue$Season==seasons[g], ]
subset <- subset[complete.cases(subset), ]
text <- str_c(subset$Line, collapse=" ")
row <- data.frame( season=seasons[g], text=text)
by.season <- rbind(by.season, row)
}
by.season[,namevector] <- NA
namevector <-c('person','location','organization')
by.season[,namevector] <- NA
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
person_ann <- Maxent_Entity_Annotator(kind = "person")
location_ann <- Maxent_Entity_Annotator(kind = "location")
organization_ann <- Maxent_Entity_Annotator(kind = "organization")
pipeline <- list(sent_ann,
word_ann,
person_ann,
location_ann,
organization_ann)
entities <- function(doc, kind) {
s <- doc$content
a <- annotations(doc)[[1]]
if(hasArg(kind)) {
k <- sapply(a$features, `[[`, "kind")
s[a[k == kind]]
} else {
s[a[a$type == "entity"]]
}
}
full_sents <- NULL
for (g in seasons){
full_dialogue = paste(by.season$text[g], collapse = " ")
nchar(full_dialogue)
full_dialogue <-as.String(full_dialogue)
first_half <- substr(full_dialogue,0,nchar(full_dialogue)/2)
second_half <- substr(full_dialogue,nchar(full_dialogue)/2, nchar(full_dialogue))
full_dialogue <- first_half
dialogue_annotations <- annotate(full_dialogue, pipeline)
dialogue_doc <- AnnotatedPlainTextDocument(full_dialogue, dialogue_annotations)
org1 <-entities(dialogue_doc, kind = "organization")
loc1 <-entities(dialogue_doc, kind = "location")
pers1<-entities(dialogue_doc, kind = "person")
full_sents <- c(full_sents, sents(dialogue_doc))
full_dialogue <- second_half
dialogue_annotations <- annotate(full_dialogue, pipeline)
dialogue_doc <- AnnotatedPlainTextDocument(full_dialogue, dialogue_annotations)
org1 <-rbind(org1,entities(dialogue_doc, kind = "organization"))
loc1 <-rbind(loc1,entities(dialogue_doc, kind = "location"))
pers1<-rbind(pers1,entities(dialogue_doc, kind = "person"))
by.season$organization[g]<-str_c(org1, collapse= ";")
by.season$location[g]<-str_c(loc1, collapse=";")
by.season$person[g]<-str_c(pers1, collapse=";")
full_sents <- c(full_sents, sents(dialogue_doc))
}
full_sents
organizations <- strsplit(by.season$organization, ";")
persons <- strsplit(by.season$person, ";")
locations <- strsplit(by.season$location, ";")
library(dplyr)
library(stringr)
library(syuzhet)
library(ggplot2)
library(viridis)
process_sentiment <- function (rawtext, mymethod) {
chunkedtext <- data_frame(x = rawtext) %>%
group_by(linenumber = ceiling(row_number() / 100)) %>%
summarize(text = str_c(x, collapse = " "))
mySentiment <- data.frame(cbind(linenumber = chunkedtext$linenumber,
sentiment = get_sentiment(chunkedtext$text, method = mymethod)))
}
xy <- process_sentiment(full_sents,"nrc")
plot_sentiment <- function (mySentiment) {
g <- ggplot(data = mySentiment, aes(x = linenumber, y = sentiment)) +
geom_bar(stat = "identity", color = "midnightblue") +
theme_minimal() +
labs(y = "Sentiment", caption = "Text sourced from Southpark Wikia") +
scale_x_discrete(expand=c(0.02,0)) +
theme(plot.caption=element_text(size=8)) +
theme(axis.text.y=element_text(margin=margin(r=-10))) +
theme(axis.title.x=element_blank()) +
theme(axis.ticks.x=element_blank()) +
theme(axis.text.x=element_blank())
}
p <- plot_sentiment(xy)
p
fourier_sentiment <- function (sentiment) {
as.numeric(get_transformed_values(sentiment[,2],
low_pass_size = 3,
scale_vals = TRUE,
scale_range = FALSE))
}
plotshape <- data_frame(linenumber = 1:100, ft = fourier_sentiment(xy)) %>% mutate(series="South Park")
pp <- ggplot(data = plotshape,aes(x = linenumber, y = ft, fill = series)) + geom_area(alpha = 0.7) +
theme_minimal() + ylab("Transformed Sentiment Value") +
labs(title = "Sentiment over 18 seasons of South Park") +xlab("Position in script")+scale_fill_viridis(end = 0.1, discrete=TRUE) +
scale_x_discrete(expand=c(0,0)) +
theme(strip.text=element_text(hjust=0)) +
theme(strip.text = element_text(face = "italic")) +
theme(plot.caption=element_text(size=9)) +
theme(legend.position="none")
pp$data$ft
pp
myReader <- readTabular(mapping=list(content="text", id="season"))
corpus <- Corpus(DataframeSource(by.season), readerControl=list(reader=myReader))
corpus <- tm_map(corpus,content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')), mc.cores=1)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removePunctuation), mc.cores=1)
corpus <- tm_map(corpus, content_transformer(removeNumbers))
corpus <- tm_map(corpus, content_transformer(stripWhitespace))
corpus <- tm_map(corpus, removeWords, stopwords("english"))
season.tdm <- TermDocumentMatrix(corpus, control = list(tokenize = allTokenizer))
allTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
season.tdm <- TermDocumentMatrix(corpus, control = list(tokenize = allTokenizer))
library(SnowballC)
m <- as.matrix(season.tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
library("wordcloud")
library("RColorBrewer")
par(bg="grey30")
png(file="WordCloud.png",width=1000,height=700, bg="grey30")
wordcloud(d$word, d$freq, col=terrain.colors(length(d$word), alpha=0.9), random.order=FALSE, rot.per=0.3 )
par(bg="grey30")
png(file="WordCloud.png",width=1000,height=700, bg="grey30")
wordcloud(d$word, d$freq, col=terrain.colors(length(d$word), alpha=0.9), random.order=FALSE, rot.per=0.3 )
title(main = "Most used words in South Park", font.main = 1, col.main = "cornsilk3", cex.main = 1.5)
dev.off()
library("wordcloud")
library("RColorBrewer")
par(bg="grey30")
png(file="WordCloud.png",width=1000,height=700, bg="grey30")
wordcloud(d$word, d$freq, col=terrain.colors(length(d$word), alpha=0.9), max.words=500, random.order=FALSE, rot.per=0.3 )
title(main = "Most used words in South Park", font.main = 1, col.main = "cornsilk3", cex.main = 1.5)
dev.off()
nrc_sents <- get_nrc_sentiment(full_sents)
nrc_sents <- get_nrc_sentiment(by.season$text)
nrc_sents <- get_nrc_sentiment(paste(by.season$text, collapse=" "))
nrc_t<- data.frame(t(nrc_sents))
View(nrc_t)
qplot(sentiment, data=nrc_t, weight=count, geom="bar",fill=sentiment)+ggtitle("Sentiments in South Park")
qplot(sentiment, data=nrc_t, weight=count, geom="bar",fill=t.nrc_sents)+ggtitle("Sentiments in South Park")
ppp
ppp <- qplot(sentiment, data=nrc_t, weight=count, geom="bar",fill=t.nrc_sents)+ggtitle("Sentiments in South Park")
ppp
View(nrc_sents)
ppp <- qplot(sentiment, data=nrc_sents, weight=count, geom="bar")+ggtitle("Sentiments in South Park")
ppp
ppp <- qplot( data=nrc_sents, weight=count, geom="bar")+ggtitle("Sentiments in South Park")
ppp <- qplot( data=nrc_sents, weight=count, geom="bar")+ggtitle("Sentiments in South Park")
ppp
sentiment <- c("anger","anticipation","disgust","fear","joy","sadness","surprise","trust")
nrc_sents <- nrc_sents[1:8,]
nrc_sents <- get_nrc_sentiment(paste(by.season$text, collapse=" "))
nrc_sents <- nrc_sents[1:8]
sentiment <- c("anger","anticipation","disgust","fear","joy","sadness","surprise","trust")
ppp <- qplot(sentiment, data=nrc_sents, weight=count, geom="bar")+ggtitle("Sentiments in South Park")
ppp
ppp <- qplot(sentiment, data=nrc_sents, weight=count, geom="bar")+ggtitle("Sentiments in South Park")
ppp <- qplot(sentiment, data=nrc_sents, weight=t.nrc_sents, geom="bar")+ggtitle("Sentiments in South Park")
ppp
ppp <- qplot(nrc_sents, data=sentiment, weight=nrc_sents, geom="bar")+ggtitle("Sentiments in South Park")
ppp
ppp <- qplot(nrc_sents, data=sentiment, weight=nrc_sents, geom="bar")+ggtitle("Sentiments in South Park")
nrc_t<- data.frame(t(nrc_sents))
names(nrc_t)[1] <- "count"
nrc_t <- cbind("sentiment" = rownames(nrc_sents), nrc_sents)
nrc_sents <- get_nrc_sentiment(paste(by.season$text, collapse=" "))
nrc_sents <- nrc_sents[1:8]
nrc_t<- data.frame(t(nrc_sents))
names(nrc_t)[1] <- "count"
nrc_t <- cbind("sentiment" = rownames(nrc_t), nrc_t)
ppp <- qplot(sentiment, data=nrc_t, weight=count, geom="bar", fill=sentiment)+ggtitle("Sentiments in South Park")
ppp
qplot(sentiment, data=nrc_t, weight=count, geom="bar", fill=sentiment)+ggtitle("Sentiments in South Park")
sentiment <- NULL
qplot(sentiment, data=nrc_t, weight=count, geom="bar", fill=sentiment)+ggtitle("Sentiments in South Park")
qplot(sentiment, data=nrc_t, weight=count, geom="bar", fill=sentiment)+ggtitle("Sentiments in South Park")
qplot(sentiment, data=nrc_t, weight=count, geom="bar")+ggtitle("Sentiments in South Park")
qplot(sentiment, y=count, data=nrc_t, weight=count, geom="bar")+ggtitle("Sentiments in South Park")
qplot(nrc_t$sentiment, y=nrc_t$count, data=nrc_t, weight=count, geom="bar")+ggtitle("Sentiments in South Park")
ppp <- ggplot(data=nrc_t, aes(x=sentiment, y=count))+geom_bar(stat="identity")
ppp
ppp <- ggplot(data=nrc_t, aes(x=nrc_t$sentiment, y=nrc_t$count))+geom_bar(stat="identity")
ppp
_t
nrc_t
df <- data.frame(dose=c("D0.5", "D1", "D2"),
len=c(4.2, 10, 29.5))
df
nrc_t <- data.frame(sentiment=nrc_t$sentiment, count = nrc_t$count)
nrc_t
ppp <- ggplot(data=nrc_t, aes(x=nrc_t$sentiment, y=nrc_t$count))+geom_bar(stat="identity")
ppp
ppp<-ggplot(data=nrc_t, aes(x=sentiment, y=count)) +
geom_bar(stat="identity")
ppp
dev.on()
dev.new()
ppp<-ggplot(data=nrc_t, aes(x=sentiment, y=count)) +
geom_bar(stat="identity")
ppp
qplot(sentiment, data=nrc_t, weight=count, geom="bar",fill=sentiment)+ggtitle("Email sentiments")
qplot(sentiment, data=nrc_t, weight=count, geom="bar",fill=sentiment)+ggtitle("Sentiments in South Park")
save.image("~/Data Analysis/files/southpark/datasetMonday.RData")
install.packages('quanteda')
collocations(textxx, size = 2:3)
(quanteda)
library(quanteda)
collocations(textxx, size = 2:3)
textxx<-paste(by.season$text, collapse=" ")
collocations(textxx, size = 2:3)
print(removeFeatures(collocations(textxx, size = 2:3), stopwords("english")))
library(tm)
library(quanteda)
textxx<-paste(by.season$text, collapse=" ")
collocations(textxx, size = 2:3)
textxx<-paste(by.season$text, collapse=" // ")
collocations(textxx, size = 2:3)
print(removeFeatures(collocations(textxx, size = 2:3), stopwords("english")))
parallelizeTask <- function(task, ...) {
# Calculate the number of cores
ncores <- detectCores() - 1
# Initiate cluster
cl <- makeCluster(ncores)
registerDoParallel(cl)
#print("Starting task")
r <- task(...)
#print("Task done")
stopCluster(cl)
r
}
makeSentences <- function(input) {
output <- tokenize(input, what = "sentence", removeNumbers = TRUE,
removePunct = TRUE, removeSeparators = TRUE,
removeTwitter = TRUE, removeHyphens = TRUE)
output <- removeFeatures(output, getProfanityWords())
unlist(lapply(output, function(a) paste('#s#', toLower(a), '#e#')))
}
makeTokens <- function(input, n = 1L) {
tokenize(input, what = "word", removeNumbers = TRUE,
removePunct = TRUE, removeSeparators = TRUE,
removeTwitter = FALSE, removeHyphens = TRUE,
ngrams = n, simplify = TRUE)
}
View(entities)
sentences <- parallelizeTask(makeSentences, corpus)
library(quanteda)
library(R.utils)
library(doParallel)
install.packages("R.utils")
(R.utils)
library(R.utils)
require(dplyr)
library(data.table)
parallelizeTask <- function(task, ...) {
# Calculate the number of cores
ncores <- detectCores() - 1
# Initiate cluster
cl <- makeCluster(ncores)
registerDoParallel(cl)
#print("Starting task")
r <- task(...)
#print("Task done")
stopCluster(cl)
r
}
makeSentences <- function(input) {
output <- tokenize(input, what = "sentence", removeNumbers = TRUE,
removePunct = TRUE, removeSeparators = TRUE,
removeTwitter = TRUE, removeHyphens = TRUE)
output <- removeFeatures(output, getProfanityWords())
unlist(lapply(output, function(a) paste('#s#', toLower(a), '#e#')))
}
makeTokens <- function(input, n = 1L) {
tokenize(input, what = "word", removeNumbers = TRUE,
removePunct = TRUE, removeSeparators = TRUE,
removeTwitter = FALSE, removeHyphens = TRUE,
ngrams = n, simplify = TRUE)
}
sentences <- parallelizeTask(makeSentences, corpus)
ngram1 <- parallelizeTask(makeTokens, sentences, 1)
ngram2 <- parallelizeTask(makeTokens, sentences, 2)
ngram3 <- parallelizeTask(makeTokens, sentences, 3)
ngram4 <- parallelizeTask(makeTokens, sentences, 4)
dfm1 <- parallelizeTask(dfm, ngram1, ignoredFeatures=getProfanityWords())
dfm2 <- parallelizeTask(dfm, ngram2, ignoredFeatures=getProfanityWords())
dfm3 <- parallelizeTask(dfm, ngram3, ignoredFeatures=getProfanityWords())
dfm4 <- parallelizeTask(dfm, ngram4, ignoredFeatures=getProfanityWords())
# Returns a vector of profanity words
getProfanityWords <- function(corpus) {
profanityFileName <- "profanity.txt"
if (!file.exists(profanityFileName)) {
profanity.url <- "https://raw.githubusercontent.com/shutterstock/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en"
download.file(profanity.url, destfile = profanityFileName, method = "curl")
}
if (sum(ls() == "profanity") < 1) {
profanity <- read.csv(profanityFileName, header = FALSE, stringsAsFactors = FALSE)
profanity <- profanity$V1
profanity <- profanity[1:length(profanity)-1]
}
profanity
}
dfm1 <- parallelizeTask(dfm, ngram1, ignoredFeatures=getProfanityWords())
dfm2 <- parallelizeTask(dfm, ngram2, ignoredFeatures=getProfanityWords())
dfm3 <- parallelizeTask(dfm, ngram3, ignoredFeatures=getProfanityWords())
dfm4 <- parallelizeTask(dfm, ngram4, ignoredFeatures=getProfanityWords())
dfm1 <- parallelizeTask(dfm, ngram1)
dfm2 <- parallelizeTask(dfm, ngram2)
dfm3 <- parallelizeTask(dfm, ngram3)
dfm4 <- parallelizeTask(dfm, ngram4)
dt4 <- data.table(ngram = features(dfm4), count = colSums(dfm4), key = "ngram")
# Store the total number of ngrams (features in quanteda terminology) for later use
nfeats <- nfeature(dfm4)
hits <- DT[ngram %like% paste("^", regex, "_", sep = ""), ngram]
dt4
dt4[10]
dt4[7]
dt4[5]
dt4[8]
dt4[9]
dt4[10]
dt1 <- data.table(ngram = features(dfm1), count = colSums(dfm1), key = "ngram")
dt1[10]
dt1
dt2 <- data.table(ngram = features(dfm2), count = colSums(dfm2), key = "ngram")
dt2
corpus <- Corpus(DataframeSource(by.season), readerControl=list(reader=myReader))
corpus <- tm_map(corpus,content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')), mc.cores=1)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removePunctuation), mc.cores=1)
corpus <- tm_map(corpus, content_transformer(removeNumbers))
corpus <- tm_map(corpus, content_transformer(stripWhitespace))
corpus <- tm_map(corpus, removeWords, stopwords("english"))
sentences <- parallelizeTask(makeSentences, corpus)
ngram1 <- parallelizeTask(makeTokens, sentences, 1)
ngram2 <- parallelizeTask(makeTokens, sentences, 2)
ngram3 <- parallelizeTask(makeTokens, sentences, 3)
ngram4 <- parallelizeTask(makeTokens, sentences, 4)
dfm1 <- parallelizeTask(dfm, ngram1)
dfm2 <- parallelizeTask(dfm, ngram2)
dfm3 <- parallelizeTask(dfm, ngram3)
dfm4 <- parallelizeTask(dfm, ngram4)
dt4 <- data.table(ngram = features(dfm4), count = colSums(dfm4), key = "ngram")
# Store the total number of ngrams (features in quanteda terminology) for later use
nfeats <- nfeature(dfm4)
dt2 <- data.table(ngram = features(dfm2), count = colSums(dfm2), key = "ngram")
hits <- DT[ngram %like% paste("^", regex, "_", sep = ""), ngram]
dt2
textxx<-paste(by.season$text, collapse=" // ")
textxx<-paste(by.season$text, collapse=" // ")
print(removeFeatures(collocations(textxx, size = 2:3), stopwords("english")))
trigram.seasonTdm <- tm::TermDocumentMatrix(corpus, control = list(tokenize = TrigramTokenizer))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
ngram_tokenizer <- function(n = 1L, skip_word_none = TRUE) {
stopifnot(is.numeric(n), is.finite(n), n > 0)
options <- stringi::stri_opts_brkiter(type="word", skip_word_none = skip_word_none)
function(x) {
stopifnot(is.character(x))
# Split into word tokens
tokens <- unlist(stringi::stri_split_boundaries(x, opts_brkiter=options))
len <- length(tokens)
if(all(is.na(tokens)) || len < n) {
# If we didn't detect any words or number of tokens is less than n return empty vector
character(0)
} else {
sapply(
1:max(1, len - n + 1),
function(i) stringi::stri_join(tokens[i:min(len, i + n - 1)], collapse = " ")
)
}
}
}
trigram.seasonTdm <- tm::TermDocumentMatrix(corpus, control = list(tokenize = TrigramTokenizer))
View(fourier_sentiment)
freq.trigram.season <- data.frame(word = trigram.seasonTdm$dimnames$Terms, frequency = trigram.seasonTdm$v)
freq.trigram.season <- plyr::arrange(freq.trigram.season, -frequency)
trigram.seasonTdm
trigram.seasonTdm
print(trigram.seasonTdm)
textxx<-paste(by.season$text, collapse=" // ")
dfmtest <- dfm(textxx, ngrams=2:3, verbose = FALSE)
inspect(dfmtest)
dfmtest
dftest <- data.frame(word=dfmtest@Dimnames$features, freq=dfmtest@x)
dftest <- arrange(dftest, -freq)
dftest
inspect(dftest)
print(dftest)
View(dftest)
View(dftest)
dfmtest <- dfm(textxx, ngrams=3:4, verbose = FALSE)
dftest <- data.frame(word=dfmtest@Dimnames$features, freq=dfmtest@x)
dftest <- arrange(dftest, -freq)
View(dftest)
View(dftest)
textxx<- gsub('[[:punct:] ]+',' ',textxx)
dfmtest <- dfm(textxx, ngrams=3:4, verbose = FALSE)
dftest <- data.frame(word=dfmtest@Dimnames$features, freq=dfmtest@x)
dftest <- arrange(dftest, -freq)
View(dftest)
View(dftest)
View(dftest)
View(dftest)
View(dftest)
textxx<- removeWords(textxx, stopwords("english"))
View(dt4)
dfmtest <- dfm(textxx, ngrams=3:4, verbose = FALSE)
dftest <- data.frame(word=dfmtest@Dimnames$features, freq=dfmtest@x)
dftest <- arrange(dftest, -freq)
View(dftest)
View(dftest)
View(dftest)
View(dftest)
textxx<-paste(by.season$text, collapse=" // ")
textxx<- gsub('[[:punct:] ]+',' ',textxx)
textxx<- removeWords(textxx, stopwords("english"))
dfmtest <- dfm(textxx, ngrams=3:4, verbose = FALSE)
dftest <- data.frame(word=dfmtest@Dimnames$features, freq=dfmtest@x)
dftest <- arrange(dftest, -freq)
View(dftest)
View(dftest)
dfmtest <- dfm(textxx, ngrams=4:4, verbose = FALSE)
dftest <- data.frame(word=dfmtest@Dimnames$features, freq=dfmtest@x)
dftest <- arrange(dftest, -freq)
View(dftest)
View(dftest)
stopwords('english')
stopwords('english')
ssss - stopwords('english')
ssss <- stopwords('english')
textxx <- tolower(textxx)
dfmtest <- dfm(textxx, ngrams=4:4, verbose = FALSE)
textxx<- removeWords(textxx, stopwords("english"))
dfmtest <- dfm(textxx, ngrams=4:4, verbose = FALSE)
dftest <- data.frame(word=dfmtest@Dimnames$features, freq=dfmtest@x)
dftest <- arrange(dftest, -freq)
View(dftest)
View(dftest)
View(dftest)
save.image("~/Data Analysis/files/southpark/datasetMonday.RData")
View(dt4)
View(dftest)
View(dftest)
View(dftest)
install.packages('igraph')
library(igraph)
ngram_graph <- dftest %>%
filter(freq > 10) %>%
graph_from_data_frame()
ngram_graph
install.packages('ggraph')
ggraph(ngram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
library(ggraph)
ggraph(ngram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
ngram_graph <- dfmtest %>%
filter(freq > 10) %>%
graph_from_data_frame()
View(plotshape)
dfmtest <- dfm(textxx, stem=TRUE, remove_punct=TRUE, ngrams=4:4, verbose = FALSE, concatenator=" ")
dftest <- data.frame(word=dfmtest@Dimnames$features, freq=dfmtest@x)
dftest <- arrange(dftest, -freq)
View(dftest)
View(dftest)
dftestsplit <- strsplit(dftest$word, " ")
dftestsplit <- strsplit(dftest$word, " ")
dftestsplit <- []
dftestsplit <- dftest$word
dftestsplit <- as.String(dftestsplit)
dftestsplit <- strsplit(dftestsplit," ")
dftestsplit <- dftest$word
dftestsplit <- as.String(dftestsplit, sep=" ")
dftestsplit <- dftest$word
dftestsplit
dftestsplit[1]
within(dftest, {
word1 <- strsplit(word," ")(1)
word2 <- strsplit(word," ")(2)
word3 <- strsplit(word," ")(3)
word4 <- strsplit(word," ")(4)
})
within(dftest, {
View(dftest)
View(dftest)
View(dftest)
View(dftest)
View(dftest)
within(dftest, {
word1 <- strsplit(dftest$word," ")(1)
word2 <- strsplit(dftest$word," ")(2)
word3 <- strsplit(dftest$word," ")(3)
word4 <- strsplit(dftest$word," ")(4)
})
View(dftest)
View(dftest)
View(dftest)
save.image("~/Data Analysis/files/southpark/datasetMonday.RData")
